---
layout: archive
title: "Research"
permalink: /research/
author_profile: true
---

<style>
table {
    border: none;
    margin-top: 5px;
}
td {
    border: none;
    font-size: 16px;
}

.paperimg{
    width: 160px;
    vertical-align: top;
}
</style>

## Publications

<table><tbody><tr>
    <td class="paperimg"><img src="/images/research-2021-training-cloud.png"></td>
    <td><b>Towards Scalable Distributed Training of Deep Learning on Public Cloud Clusters</b>
    <br>Shaohuai Shi, Xianhao Zhou, Shutao Song, <u>Xingyao Wang</u>, Zilin Zhu, Xue Huang, Xinan Jiang, Feihu Zhou, Zhenyu Guo, Liqiang Xie, Rui Lan, Xianbin Ouyang, Yan Zhang, Jieqian Wei, Jing Gong, Weiliang Lin, Ping Gao, Peng Meng, Xiaomin Xu, Chenyang Guo, Bo Yang, Zhibo Chen, Yongjian Wu, Xiaowen Chu
    <br>Proceedings of Machine Learning and Systems (MLSys), 2021. <b>Under Review</b>
    <br><a href="https://arxiv.org/abs/2010.10458">link</a></td>
<!-- &nbsp;·&nbsp; -->
</tr></tbody></table>

<table><tbody><tr>
    <td class="paperimg"><img src="/images/research-2020-hough.png"></td>
    <td><b>Lane Extraction and Quality Evaluation: A Hough Transform Based Approach</b>
    <br><u>Xingyao Wang</u>, Da Yan, Ke Chen, Yancong Deng, Cheng Long, Kunlin Zhang, Sibo Yan
    <br>Proceedings of IEEE Conference on Multimedia Information Processing and Retrieval (MIPR), 2020.
    <br><a href="https://ieeexplore.ieee.org/document/9175569">link</a>&nbsp;·&nbsp;
    <a href="https://github.com/xingyaoww/HT-Based-Evaluation-Metric">code</a>
    <!-- &nbsp;·&nbsp; -->
    </td>
</tr></tbody></table>

## Ongoing Projects

<table><tbody><tr>
    <td class="paperimg">
    <!-- <iframe src="https://giphy.com/embed/a9gu5GIJGJ9du" width="160" frameBorder="0" class="giphy-embed" allowFullScreen></iframe> -->
    <iframe src="https://giphy.com/embed/g9582DNuQppxC" width="160" frameBorder="0" class="giphy-embed" allowFullScreen></iframe>
    </td>
    <td><b>GIF Reply</b>
    <br><u>Xingyao Wang</u>, David Jurgens
    <br><b>Ongoing</b>
    <br>
    <details><summary><span title="Click the text for more information">How is multimodal media used as a language to communicate in social media? Given enough data, can we train a model to appropriately respond to text-based content using other modalities? To answer these questions, I am working on the multimodal language problem: Twitter GIF reply. </span></summary> 
    GIF, a series of images or soundless videos that loops, are prevalent on Twitter nowadays. Widely used in response to tweets, easily shareable GIFs enable the sender to add context or emotions to short replies in a fun and easy to understand way. 
    <br>I formulate interactions between English-language tweets and their GIF responses as pseudo-multilingual conversations between English and the “GIF language”: a special vision-based language. Reply interactions between GIFs and tweets can be thought of as an interaction between the English language and the “GIF language”. I constructed a dataset containing 1.6M English tweets and GIF replies, and I am exploring multimodal Transformers as a tool to learn the mapping between the language of tweets and semantics of the GIFs.</details>
    <!-- &nbsp;·&nbsp; -->
</td></tr></tbody></table>
